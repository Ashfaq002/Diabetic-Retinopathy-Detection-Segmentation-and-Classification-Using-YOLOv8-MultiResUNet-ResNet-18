# -*- coding: utf-8 -*-
"""II_500_1133BBCC_CLASSIFI_DRET_YOLOv8_detectionandSegementation1AUGULTIRESANDCLASSIFICEPOCHS500__.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X-Elgnl7AJRFqrwrPAvI_JS-JkxtAklk
"""

!pip install ultralytics

from google.colab import drive
drive.mount('/content/drive')

from ultralytics import YOLO
print("YOLOv8 installed successfully!")

from ultralytics import YOLO

# Load the YOLOv8 model (pretrained weights)
model = YOLO('yolov8s.pt')  # 'yolov8s.pt' is the small YOLOv8 model

# Train on your dataset
model.train(
    data='/content/drive/MyDrive/retinopatia diabetica.v1i.yolov8/data.yaml',  # Path to your dataset YAML file
    epochs=200,                                   # Number of epochs
    imgsz=640,                                   # Image size
    batch=8                                      # Batch size
)

import cv2
import os
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO

# Load trained YOLOv8 Object Detection model
model = YOLO("/content/drive/MyDrive/DREyolov8_experiment/content/runs/detect/train2/weights/best.pt")  # Update the path to your trained model

# Define test image folder path
test_folder = "/content/drive/MyDrive/YOLOV8-SEGMENTATION DATASET/images/test"  # Update with your test images folder path

# Get list of test images
test_images = [f for f in os.listdir(test_folder) if f.endswith((".jpg", ".png", ".jpeg"))]

# Process and visualize predictions for test images
for filename in test_images:
    image_path = os.path.join(test_folder, filename)

    # Load original image
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Run YOLOv8 object detection
    results = model(image)

    # Create a copy for visualization
    output_image = image_rgb.copy()

    # Process results
    for result in results:
        boxes = result.boxes.xyxy.cpu().numpy()  # Bounding box coordinates
        class_ids = result.boxes.cls.cpu().numpy()  # Class labels
        confs = result.boxes.conf.cpu().numpy()  # Confidence scores

        # Loop through detected objects
        for box, class_id, conf in zip(boxes, class_ids, confs):
            x1, y1, x2, y2 = map(int, box)
            label = f"{model.names[int(class_id)]} ({conf:.2f})"  # Class label with confidence

            # Draw bounding box
            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Put class label
            cv2.putText(output_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                        0.8, (0, 255, 0), 2, cv2.LINE_AA)

    # Display the original and YOLO-predicted images side by side
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # Original Image
    axes[0].imshow(image_rgb)
    axes[0].set_title("Original Test Image")
    axes[0].axis("on")  # Add axis lines

    # YOLO Predicted Image
    axes[1].imshow(output_image)
    axes[1].set_title("YOLO Predicted Image")
    axes[1].axis("on")  # Add axis lines

    plt.show()

import cv2
import os
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
import random

# Load trained YOLOv8 Object Detection model
model = YOLO("/content/drive/MyDrive/DREyolov8_experiment/content/runs/detect/train2/weights/best.pt")  # Update path

# Define test image folder path
test_folder = "/content/drive/MyDrive/YOLOV8-SEGMENTATION DATASET/images/test"  # Update path

# Get list of test images
test_images = [f for f in os.listdir(test_folder) if f.endswith((".jpg", ".png", ".jpeg"))]

# Pick 4 random images (or all if less than 4 available)
num_images = min(4, len(test_images))  # Ensure we don't exceed available images
random_images = random.sample(test_images, num_images)  # Select random images

# Define subplot layout based on the number of images
fig, axes = plt.subplots(1, num_images, figsize=(15, 6))

# Process and visualize predictions for selected test images
for idx, filename in enumerate(random_images):
    image_path = os.path.join(test_folder, filename)

    # Load original image
    image = cv2.imread(image_path)
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

    # Run YOLOv8 object detection
    results = model(image)

    # Create a copy for visualization
    output_image = image_rgb.copy()

    # Process results
    for result in results:
        boxes = result.boxes.xyxy.cpu().numpy()  # Bounding box coordinates
        class_ids = result.boxes.cls.cpu().numpy()  # Class labels
        confs = result.boxes.conf.cpu().numpy()  # Confidence scores

        # Loop through detected objects
        for box, class_id, conf in zip(boxes, class_ids, confs):
            x1, y1, x2, y2 = map(int, box)
            label = f"{model.names[int(class_id)]} ({conf:.2f})"  # Class label with confidence

            # Draw bounding box
            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Put class label
            cv2.putText(output_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                        0.6, (0, 255, 0), 2, cv2.LINE_AA)

    # Display each image in the subplot
    axes[idx].imshow(output_image)
    axes[idx].set_title("Prediction")
    axes[idx].axis("on")  # Add axis lines

# Adjust layout
plt.tight_layout()
plt.show()

import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding
!zip -r DREyolov8_experiment.zip /content/runs

# Validate the trained model on the dataset
metrics = model.val()

# Print mAP, Precision, Recall, and F1-score
print(f"mAP50: {metrics.box.map50:.4f}")         # mAP at 0.5 IoU
print(f"mAP50-95: {metrics.box.map:.4f}") # mAP at IoU 0.5:0.95 # Changed map50_95 to map
print(f"Precision: {metrics.box.p.mean():.4f}")  # Calculate and print the mean precision
print(f"Recall: {metrics.box.r.mean():.4f}")  # Calculate and print the mean recall
print(f"F1-score: {metrics.box.f1.mean():.4f}")  # Calculate and print the mean F1-score

#-----SEGEMENTATION & MASK GENERATION----------#

import cv2
import os
import numpy as np

# Paths
mask_folder = "/content/drive/MyDrive/TEST-MASKS"  # Folder containing binary masks
output_folder = "/content/drive/MyDrive/TEST-LABELS"  # Folder to save polygon TXT files

# Create output folder if it doesn't exist
os.makedirs(output_folder, exist_ok=True)

for filename in os.listdir(mask_folder):
    if filename.endswith((".png", ".jpg", ".jpeg")):
        mask_path = os.path.join(mask_folder, filename)
        output_path = os.path.join(output_folder, filename.replace(".png", ".txt"))  # Save as TXT

        # Load binary mask (grayscale)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        # Find contours (polygons)
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        with open(output_path, "w") as f:
            for contour in contours:
                if len(contour) < 3:  # Ignore very small contours
                    continue

                # Normalize polygon points (convert to YOLO format)
                polygon = []
                for point in contour:
                    x, y = point[0]
                    polygon.append(f"{x / mask.shape[1]:.6f} {y / mask.shape[0]:.6f}")

                # Example class ID: Assign your class ID dynamically (modify as needed)
                class_id = 0  # Change this according to your dataset

                # Write polygon annotation in YOLO format
                f.write(f"{class_id} {' '.join(polygon)}\n")

        print(f"Saved: {output_path}")

print("✅ Polygon conversion completed!")

import cv2
import os
import numpy as np
import matplotlib.pyplot as plt

# Define paths for test images and annotation TXT files
test_image_folder = "/content/drive/MyDrive/YOLOV8-SEGMENTATION DATASET/images/test"
annotation_folder = "/content/drive/MyDrive/TEST-LABELS"

# Get list of test images
test_images = [f for f in os.listdir(test_image_folder) if f.endswith((".jpg", ".png", ".jpeg"))]

# Define fixed image dimensions
IMAGE_WIDTH = 250
IMAGE_HEIGHT = 250

# Process and visualize images
for filename in test_images:
    # Paths for image and corresponding annotation TXT file
    image_path = os.path.join(test_image_folder, filename)
    annotation_path = os.path.join(annotation_folder, filename.replace(".jpg", ".txt").replace(".png", ".txt").replace(".jpeg", ".txt"))

    # Load the original image
    original_image = cv2.imread(image_path)
    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
    original_image_rgb = cv2.resize(original_image_rgb, (IMAGE_WIDTH, IMAGE_HEIGHT))  # Resize to 250x250

    # Create an empty black mask
    mask_binary = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH), dtype=np.uint8)

    # Check if annotation file exists
    if os.path.exists(annotation_path):
        with open(annotation_path, "r") as file:
            for line in file.readlines():
                data = line.strip().split()
                class_id = int(data[0])  # YOLO format: First value is class ID
                polygon_points = np.array(data[1:], dtype=np.float32).reshape(-1, 2)

                # Convert normalized coordinates (YOLO format) to actual image size
                polygon_points[:, 0] *= IMAGE_WIDTH
                polygon_points[:, 1] *= IMAGE_HEIGHT
                polygon_points = polygon_points.astype(np.int32)

                # Draw filled polygon on the mask
                cv2.fillPoly(mask_binary, [polygon_points], 255)

    # Create an overlay: Convert single-channel mask to 3-channel for visualization
    overlay = np.zeros_like(original_image_rgb)
    overlay[:, :, 1] = mask_binary  # Green color for mask overlay

    # Blend mask with original image (Transparency 50%)
    overlayed_image = cv2.addWeighted(original_image_rgb, 1, overlay, 0.5, 0)

    # Display images side by side without grid lines
    fig, axes = plt.subplots(1, 2, figsize=(8, 4))

    # Original Image with Mask Overlay
    axes[0].imshow(overlayed_image)
    axes[0].set_title("Original Image with Mask Overlay")
    axes[0].set_xticks(np.arange(0, 251, 50))
    axes[0].set_yticks(np.arange(0, 251, 50))
    axes[0].set_xlim(0, 250)
    axes[0].set_ylim(250, 0)

    # Segmentation Mask
    axes[1].imshow(mask_binary, cmap="gray")
    axes[1].set_title("Segmentation Mask")
    axes[1].set_xticks(np.arange(0, 251, 50))
    axes[1].set_yticks(np.arange(0, 251, 50))
    axes[1].set_xlim(0, 250)
    axes[1].set_ylim(250, 0)

    plt.show()

import cv2
import os
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO

# Load trained YOLOv8 Object Detection model
obj_det_model = YOLO("/content/drive/MyDrive/DREyolov8_experiment/content/runs/detect/train2/weights/best.pt")

# Define paths for test images and segmentation annotation files
test_image_folder = "/content/drive/MyDrive/YOLOV8-SEGMENTATION DATASET/images/test"
mask_annotation_folder = "/content/drive/MyDrive/TEST-LABELS"  # Folder containing YOLO TXT mask annotations

# Get list of test images
test_images = [f for f in os.listdir(test_image_folder) if f.endswith((".jpg", ".png", ".jpeg"))]

# Function to read YOLO annotation and create a mask
def create_mask_from_yolo(yolo_annotation_path, image_shape):
    height, width = image_shape[:2]

    # Initialize an empty mask
    mask = np.zeros((height, width), dtype=np.uint8)

    if not os.path.exists(yolo_annotation_path):
        print(f"Annotation file missing: {yolo_annotation_path}")
        return mask

    with open(yolo_annotation_path, "r") as file:
        for line in file:
            data = line.strip().split()
            class_id = int(data[0])  # Class label
            points = np.array([float(x) for x in data[1:]]).reshape(-1, 2)

            # Convert YOLO normalized coordinates to pixel coordinates
            points[:, 0] = points[:, 0] * width
            points[:, 1] = points[:, 1] * height
            points = points.astype(np.int32)

            # Draw filled polygon (segmentation mask)
            cv2.fillPoly(mask, [points], 255)  # White mask for detected region

    return mask

# Process and visualize images
for filename in test_images:
    image_path = os.path.join(test_image_folder, filename)
    annotation_path = os.path.join(mask_annotation_folder, filename.replace(".jpg", ".txt").replace(".png", ".txt"))

    # Load the original image
    original_image = cv2.imread(image_path)
    original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)

    # Generate segmentation mask from annotation TXT
    mask_binary = create_mask_from_yolo(annotation_path, original_image.shape)

    # Create an overlay: Convert single-channel mask to 3-channel for visualization
    overlay = np.zeros_like(original_image_rgb)
    overlay[:, :, 1] = mask_binary  # Green overlay for mask

    # Blend mask with original image (Transparency 50%)
    overlayed_image = cv2.addWeighted(original_image_rgb, 1, overlay, 0.5, 0)

    # Run YOLOv8 object detection
    obj_results = obj_det_model(original_image)

    # Copy original image for visualization
    obj_output_image = original_image_rgb.copy()

    # Process detection results
    for result in obj_results:
        boxes = result.boxes.xyxy.cpu().numpy()  # Bounding box coordinates
        class_ids = result.boxes.cls.cpu().numpy()  # Class labels
        confs = result.boxes.conf.cpu().numpy()  # Confidence scores

        for box, class_id, conf in zip(boxes, class_ids, confs):
            x1, y1, x2, y2 = map(int, box)
            label = f"{obj_det_model.names[int(class_id)]} ({conf:.2f})"

            # Draw bounding box
            cv2.rectangle(obj_output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(obj_output_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)

    # Display images side by side
    plt.figure(figsize=(18, 6))

    # Object Detection Image (Bounding Boxes)
    plt.subplot(1, 3, 1)
    plt.imshow(obj_output_image)
    plt.title("Object Detection: Bounding Boxes")
    plt.axis("on")  # Add axis lines

    # Segmentation Overlay Image
    plt.subplot(1, 3, 2)
    plt.imshow(overlayed_image)
    plt.title("Original Image with Mask Overlay")
    plt.axis("on")  # Add axis lines

    # Black & White Mask
    plt.subplot(1, 3, 3)
    plt.imshow(mask_binary, cmap="gray")
    plt.title("Segmentation Mask")
    plt.axis("on")  # Add axis lines

    plt.show()

    # Stop after showing one image (Remove this break to process all images)
    # break

!apt-get update
!apt-get install -y --allow-downgrades cuda-11-8

# Install the correct cuDNN version
!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcudnn8_8.9.4.25-1+cuda11.8_amd64.deb
!dpkg -i libcudnn8_8.9.4.25-1+cuda11.8_amd64.deb

# Restart runtime
import os
os.kill(os.getpid(), 9)

import gc
import tensorflow as tf

gc.collect()
tf.keras.backend.clear_session()

!nvcc --version

!pip install tensorflow==2.15.1

import tensorflow as tf

print("TensorFlow version:", tf.__version__)
print("Num GPUs Available:", len(tf.config.list_physical_devices('GPU')))

!pip install albumentations

import os
import cv2
import numpy as np
import albumentations as A
from albumentations.core.composition import OneOf

# Paths
test_image_folder = "/content/drive/MyDrive/YOLOV8-SEGMENTATION DATASET/images/test"
test_mask_folder = "/content/drive/MyDrive/YOLO-SEGMENT2V8-OUTPUT-MASKS-20250215T041223Z-001/YOLO-SEGMENT2V8-OUTPUT-MASKS"

augmented_image_folder = "/content/drive/MyDrive/AUGMENTED-DATASET/BBQQ-YOLOV8-AUG-IMAGE-SEGMENT500"
augmented_mask_folder = "/content/drive/MyDrive/AUGMENTED-DATASET/BBQQ33-YOLOV8-AUG-MASK-SEGMENT500"

# Create output folders if they don't exist
os.makedirs(augmented_image_folder, exist_ok=True)
os.makedirs(augmented_mask_folder, exist_ok=True)

# List test images
test_images = [f for f in os.listdir(test_image_folder) if f.endswith((".jpg", ".png", ".jpeg"))]

# Define Augmentations
augmentations = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    OneOf([
        A.RandomBrightnessContrast(p=0.5),
        A.GaussianBlur(blur_limit=3, p=0.5),
        A.ElasticTransform(alpha=1, sigma=50, p=0.5),  # Removed 'alpha_affine'
    ], p=0.8),
    A.Resize(512, 512),  # Resize to a fixed size (optional)
], additional_targets={'mask': 'image'})  # Ensures masks align with images

# Augment up to 500 images
total_images_needed = 500
existing_count = len(test_images)
augmentations_per_image = (total_images_needed // existing_count) + 1  # Augment each image multiple times

image_count = 0  # Counter for new images

for filename in test_images:
    image_path = os.path.join(test_image_folder, filename)

    # Find the correct mask filename (assuming same base name)
    base_filename, ext = os.path.splitext(filename)
    possible_mask_filenames = [f"{base_filename}.png", f"{base_filename}_mask.png"]

    mask_path = None
    for mask_filename in possible_mask_filenames:
        possible_mask_path = os.path.join(test_mask_folder, mask_filename)
        if os.path.exists(possible_mask_path):
            mask_path = possible_mask_path
            break

    # If no corresponding mask found, skip
    if not mask_path:
        print(f"❌ Skipping {filename} - Mask not found!")
        continue

    # Load original image and mask
    image = cv2.imread(image_path)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

    # If image or mask fails to load, skip
    if image is None or mask is None:
        print(f"❌ Skipping {filename} - Image or mask loading failed!")
        continue

    # Save original image and mask with same filename
    cv2.imwrite(os.path.join(augmented_image_folder, filename), image)
    cv2.imwrite(os.path.join(augmented_mask_folder, filename), mask)
    image_count += 1

    # Apply augmentations multiple times to reach 500 images
    for i in range(augmentations_per_image):
        if image_count >= total_images_needed:
            break  # Stop when 500 images are reached

        augmented = augmentations(image=image, mask=mask)
        aug_image = augmented["image"]
        aug_mask = augmented["mask"]

        # Generate augmented filename with extension
        aug_filename = f"{base_filename}_aug_{i}.png"

        # Save augmented image and mask
        cv2.imwrite(os.path.join(augmented_image_folder, aug_filename), aug_image)
        cv2.imwrite(os.path.join(augmented_mask_folder, aug_filename), aug_mask)

        print(f"✅ Saved {aug_filename} - Augmented")
        image_count += 1

    if image_count >= total_images_needed:
        break  # Stop once 200 images are saved

print(f"✅ Augmentation Completed! {image_count} Images and Masks Saved.")

## Imports
import os
import sys
import random

import numpy as np
import cv2
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow import keras



## Seeding
seed = 2019
random.seed = seed
np.random.seed = seed
tf.seed = seed

image_size = 256
train_path = "/content/drive/MyDrive/"
epochs = 500
batch_size = 8

import os
import cv2
import numpy as np
from tensorflow import keras

class DataGen(keras.utils.Sequence):
    def __init__(self, ids, path, batch_size, image_size=128):
        self.batch_size = batch_size
        self.image_size = image_size
        self.ids = ids
        self.mask_ids = ids
        self.path = path
        self.on_epoch_end()

    def __load__(self, id_name, mask_id_name):
        # Construct image and mask paths using f-strings for better readability and debugging
        image_path = os.path.join(self.path, f"AAQQ-YOLOV8-AUG-IMAGE-SEGMENT-20250215T051918Z-001/AAQQ-YOLOV8-AUG-IMAGE-SEGMENT/{id_name}")
        mask_path = os.path.join(self.path, f"AAQQ22-YOLOV8-AUG-MASK-SEGMENT-20250215T051918Z-001/AAQQ22-YOLOV8-AUG-MASK-SEGMENT/{mask_id_name}")

        # Print paths for debugging
        #print(f"Image path: {image_path}")
        #print(f"Mask path: {mask_path}")

        image = cv2.imread(image_path, 1)
        mask = cv2.imread(mask_path, 1)

        # Check if images and masks are loaded correctly before proceeding
        if image is None:
            raise ValueError(f"Error loading image: {image_path}")
        if mask is None:
            raise ValueError(f"Error loading mask: {mask_path}")

        image = cv2.resize(image, (self.image_size, self.image_size))
        mask = cv2.resize(mask, (self.image_size, self.image_size))
        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
        mask = np.expand_dims(mask, axis=-1)

        image = image / 255.0
        mask = mask / 255.0

        return image, mask
    # ... (rest of the code remains the same)

    def __getitem__(self, index):
        if (index + 1) * self.batch_size > len(self.ids):
            self.batch_size = len(self.ids) - index * self.batch_size

        files_batch = self.ids[index * self.batch_size: (index + 1) * self.batch_size]
        mask_files_batch = self.mask_ids[index * self.batch_size: (index + 1) * self.batch_size]

        image = []
        mask = []

        for id_name, mask_id_name in zip(files_batch, mask_files_batch):
            _img, _mask = self.__load__(id_name, mask_id_name)
            image.append(_img)
            mask.append(_mask)

        image = np.array(image)
        mask = np.array(mask)

        return image, mask

    def on_epoch_end(self):
        pass

    def __len__(self):
        return int(np.ceil(len(self.ids) / float(self.batch_size)))


class TestGen(keras.utils.Sequence):
    def __init__(self, ids, path, batch_size, image_size=128):
        self.batch_size = 8
        self.image_size = image_size
        self.ids = ids
        self.path = path

    def __load__(self, id_name):
        image_path = os.path.join(self.path, "AAQQ-YOLOV8-AUG-IMAGE-SEGMENT-20250215T051918Z-001/AAQQ-YOLOV8-AUG-IMAGE-SEGMENT/", id_name)
        mask_path = os.path.join(self.path, "AAQQ22-YOLOV8-AUG-MASK-SEGMENT-20250215T051918Z-001/AAQQ22-YOLOV8-AUG-MASK-SEGMENT/", id_name)

        image = cv2.imread(image_path, 1)
        image = cv2.resize(image, (self.image_size, self.image_size))
        image = image / 255.0

        mask = cv2.imread(mask_path, 1)
        mask = cv2.resize(mask, (self.image_size, self.image_size))
        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)
        mask = np.expand_dims(mask, axis=-1)
        mask = mask / 255.0

        return image, mask

    def __getitem__(self, index):
        if (index + 1) * self.batch_size > len(self.ids):
            self.batch_size = len(self.ids) - index * self.batch_size

        files_batch = self.ids[index * self.batch_size: (index + 1) * self.batch_size]

        image = []
        mask = []

        for id_name in files_batch:
            _img, _mask = self.__load__(id_name)
            image.append(_img)
            mask.append(_mask)

        image = np.array(image)
        mask = np.array(mask)

        return image, mask

    def __len__(self):
        return int(np.ceil(len(self.ids) / float(self.batch_size)))

import os
import random  # Ensure random is correctly imported

# Path to your augmented images folder
augmented_images_folder = "/content/drive/MyDrive/AAQQ-YOLOV8-AUG-IMAGE-SEGMENT-20250215T051918Z-001/AAQQ-YOLOV8-AUG-IMAGE-SEGMENT"

# Get all filenames (assuming they are the same in images and masks folder)
all_filenames = sorted([f for f in os.listdir(augmented_images_folder) if f.endswith((".jpg", ".png", ".jpeg"))])

# Check if files are correctly loaded
if not all_filenames:
    print("Error: No image files found in the folder!")
else:
    print(f"Found {len(all_filenames)} image files.")

# Shuffle the filenames for randomness
#random.seed(42)  # Ensure reproducibility
random.shuffle(all_filenames)

# Split into 70% Train, 20% Validation, 10% Test
total_images = len(all_filenames)
train_split = int(0.8 * total_images)
valid_split = int(0.1 * total_images)

train_ids = all_filenames[:train_split]          # 70% (350 images)
valid_ids = all_filenames[train_split:train_split + valid_split]  # 20% (100 images)
test_ids = all_filenames[train_split + valid_split:]  # 10% (50 images)

# Print counts to verify
print(f"Total Images: {total_images}")
print(f"Training Set: {len(train_ids)} images")
print(f"Validation Set: {len(valid_ids)} images")
print(f"Testing Set: {len(test_ids)} images")

for i in range(len(test_ids)):  # Use the shortest list to control the loop
    print(valid_ids[i], train_ids[i], test_ids[i])

gen = DataGen(train_ids, train_path, batch_size, image_size=image_size)
x, y = gen.__getitem__(0)
print(x.shape, y.shape)

test_gen = TestGen(test_ids, train_path, batch_size, image_size=image_size)
X,Y = test_gen.__getitem__(0)
print(X.shape, Y.shape)

r = random.randint(0, len(x)-1)

fig = plt.figure()
fig.subplots_adjust(hspace=0.4, wspace=0.4)
ax = fig.add_subplot(1, 2, 1)
ax.imshow(x[r])
ax = fig.add_subplot(1, 2, 2)
ax.imshow(np.reshape(y[r], (image_size, image_size)), cmap="gray")



from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Conv2DTranspose
from tensorflow.keras.layers import Concatenate, Input
from tensorflow.keras.models import Model

def conv_block(x, num_filters, kernel_size, padding="same", act=True):
    x = tf.keras.layers.Conv2D(num_filters, kernel_size, padding=padding, use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    if act:
        x = tf.keras.layers.Activation("relu")(x)
    return x

def multires_block(x, num_filters, alpha=1.67):
    W = num_filters * alpha

    x0 = x
    x1 = conv_block(x0, int(W*0.167), 3)
    x2 = conv_block(x1, int(W*0.333), 3)
    x3 = conv_block(x2, int(W*0.5), 3)
    xc = tf.keras.layers.Concatenate()([x1, x2, x3])
    xc = tf.keras.layers.BatchNormalization()(xc)

    nf = int(W*0.167) + int(W*0.333) + int(W*0.5)
    sc = conv_block(x0, nf, 1, act=False)

    x = tf.keras.layers.Activation("relu")(xc + sc)
    x = tf.keras.layers.BatchNormalization()(x)
    return x

def res_path(x, num_filters, length):
    for i in range(length):
        x0 = x
        x1 = conv_block(x0, num_filters, 3, act=False)
        sc = conv_block(x0, num_filters, 1, act=False)
        x = tf.keras.layers.Activation("relu")(x1 + sc)
        x = tf.keras.layers.BatchNormalization()(x)
    return x

def encoder_block(x, num_filters, length):
    x = multires_block(x, num_filters)
    s = res_path(x, num_filters, length)
    p = tf.keras.layers.MaxPooling2D((2, 2))(x)
    return s, p

def decoder_block(x, skip, num_filters):
    x = tf.keras.layers.Conv2DTranspose(num_filters, 2, strides=2, padding="same")(x)
    x = tf.keras.layers.Concatenate()([x, skip])
    x = multires_block(x, num_filters)
    return x

def build_multiresunet(shape):
    """ Input """
    inputs = tf.keras.layers.Input(shape)

    """ Encoder """
    p0 = inputs
    s1, p1 = encoder_block(p0, 32, 4)
    s2, p2 = encoder_block(p1, 64, 3)
    s3, p3 = encoder_block(p2, 128, 2)
    s4, p4 = encoder_block(p3, 256, 1)

    """ Bridge """
    b1 = multires_block(p4, 512)

    """ Decoder """
    d1 = decoder_block(b1, s4, 256)
    d2 = decoder_block(d1, s3, 128)
    d3 = decoder_block(d2, s2, 64)
    d4 = decoder_block(d3, s1, 32)

    """ Output """
    outputs = tf.keras.layers.Conv2D(1, 1, padding="same", activation="sigmoid")(d4)

    """ Model """
    model = tf.keras.models.Model(inputs, outputs, name="MultiResUNET")

    return model

smooth = 1.
import tensorflow as tf
from tensorflow.keras import backend as K # Changed import

def dice_coef(y_true, y_pred):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)

    # Cast y_true_f to float32 to match y_pred_f
    y_true_f = tf.cast(y_true_f, dtype=tf.float32)

    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)


def dice_coef_loss(y_true, y_pred):
    return 1.0 - dice_coef(y_true, y_pred)

shape = (256, 256, 3)
adam = keras.optimizers.Adam()
model = build_multiresunet(shape)
model.compile(optimizer=adam, loss=dice_coef_loss, metrics=[dice_coef])
model.summary()

train_gen = DataGen(train_ids, train_path, image_size=image_size, batch_size=batch_size)
valid_gen = DataGen(valid_ids, train_path, image_size=image_size, batch_size=batch_size)
test_gen = TestGen(test_ids, train_path, image_size=image_size, batch_size=8)
# Determine the number of steps per epoch
train_steps = len(train_ids) // batch_size
valid_steps = len(valid_ids) // batch_size
test_steps = len(test_ids) // batch_size

# If the length of the data is not evenly divisible by the batch size, add 1 additional step
if len(train_ids) % batch_size != 0:
    train_steps += 1
if len(valid_ids) % batch_size != 0:
    valid_steps += 1
if len(test_ids) % batch_size != 0:
    test_steps += 1

epochs = 500

history = model.fit(train_gen, validation_data=valid_gen, validation_steps=valid_steps,
                              epochs=epochs, steps_per_epoch=train_steps, verbose=1)

# Evaluate the model on the test data
test_results = model.evaluate(test_gen, steps=test_steps)
print("Test Loss:", test_results[0])
print("Test Accuracy:", test_results[1])

model.save_weights("/content/drive/MyDrive/YOLOV8_seg-MulTiresunet500.weights.h5")

HISTORY =  history

model.save("/content/drive/MyDrive/YOLO-multiresunet_500_epochs.h5")

print("\n      Ground Truth            Predicted Value")

for i in range(test_steps):
    # Dataset for prediction
    x, y = test_gen.__getitem__(i)

    # Predict segmentation masks using the trained model
    result = model.predict(x)
    result = result > 0.2

    for i in range(len(result)):
        fig = plt.figure()
        fig.subplots_adjust(hspace=0.4, wspace=0.4)

        ax = fig.add_subplot(1, 2, 1)
        ax.imshow(np.reshape(y[i]*255, (image_size, image_size)), cmap="gray")
        ax.set_title("Ground Truth mask")

        ax = fig.add_subplot(1, 2, 2)
        ax.imshow(np.reshape(result[i]*255, (image_size, image_size)), cmap="gray")
        ax.set_title("Predicted mask")



import matplotlib.pyplot as plt

# Extracting training and validation loss from the history object
train_loss = HISTORY.history['loss']
val_loss = HISTORY.history['val_loss']

# Extracting training and validation dice coefficient from the history object
train_dice = HISTORY.history['dice_coef']
val_dice = HISTORY.history['val_dice_coef']

epochs = range(1, len(train_loss) + 1)

# Creating subplots: 1 row, 2 columns
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Plot Training & Validation Loss
axes[0].plot(epochs, train_loss, 'b', label='Training Loss')
axes[0].plot(epochs, val_loss, 'r', label='Validation Loss')
axes[0].set_title('Training vs. Validation Loss')
axes[0].set_xlabel('Epochs')
axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid()

# Plot Training & Validation Dice Coefficient
axes[1].plot(epochs, train_dice, 'b', label='Training Dice Coef')
axes[1].plot(epochs, val_dice, 'r', label='Validation Dice Coef')
axes[1].set_title('Training vs. Validation Dice Coefficient')
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('Dice Coefficient')
axes[1].legend()
axes[1].grid()

# Show the plots
plt.tight_layout()
plt.show()

import numpy as np

def calculate_miss_rate(y_true, y_pred):
    # Flatten the ground truth and predicted masks
    y_true_f = y_true.flatten()

    # Threshold y_pred to obtain binary masks
    y_pred_binary = (y_pred > 0.5).astype(np.int32)
    y_pred_f = y_pred_binary.flatten()

    # True negatives (TN): Ground truth negative and prediction negative
    true_negatives = np.sum((1 - y_true_f) * (1 - y_pred_f))

    # False negatives (FN): Ground truth positive and prediction negative
    false_negatives = np.sum(y_true_f * (1 - y_pred_f))

    # Miss rate = FN / (FN + TN)
    miss_rate = false_negatives / (false_negatives + true_negatives + np.finfo(float).eps)

    return miss_rate

def calculate_specificity(y_true, y_pred):
    # Convert predictions to binary values
    y_pred_binary = (y_pred > 0.5).astype(np.int32)

    # True negatives: actual negatives correctly identified
    tn = np.sum((y_true == 0) & (y_pred_binary == 0))

    # False positives: actual negatives incorrectly identified as positives
    fp = np.sum((y_true == 0) & (y_pred_binary == 1))

    # Specificity = TN / (TN + FP)
    specificity = tn / (tn + fp + np.finfo(float).eps)

    return specificity

def calculate_npv(y_true, y_pred):
    # Convert predicted values to binary (0 or 1)
    y_pred_binary = (y_pred > 0.5).astype(int)

    # Calculate true negatives (TN), false negatives (FN)
    # true positives (TP), false positives (FP)
    TN = np.sum((1 - y_true) * (1 - y_pred_binary))
    FN = np.sum(y_true * (1 - y_pred_binary))
    TP = np.sum(y_true * y_pred_binary)
    FP = np.sum((1 - y_true) * y_pred_binary)

    # Calculate Negative Predictive Value (NPV)
    NPV = TN / (TN + FN + np.finfo(float).eps)

    return NPV

def calculate_fall_out(y_true, y_pred):
    # Flatten the ground truth and predicted masks
    y_true_f = y_true.flatten()
    y_pred_f = y_pred.flatten()

    # Convert the predicted probabilities to binary predictions
    y_pred_binary = (y_pred_f > 0.5).astype(int)

    # Calculate confusion matrix
    tp = np.sum((y_true_f == 1) & (y_pred_binary == 1))
    tn = np.sum((y_true_f == 0) & (y_pred_binary == 0))
    fp = np.sum((y_true_f == 0) & (y_pred_binary == 1))
    fn = np.sum((y_true_f == 1) & (y_pred_binary == 0))

    # Calculate fall-out
    fall_out = fp / (fp + tn + np.finfo(float).eps)  # Add a small value to avoid division by zero

    return fall_out

def pixel_accuracy(y_true, y_pred):
    # Convert predictions to binary masks
    y_pred = (y_pred > 0.5).astype(int)

    # Calculate pixel-wise accuracy
    correct_pixels = np.sum((y_true == y_pred).astype(int))
    total_pixels = np.prod(y_true.shape)
    accuracy = correct_pixels / total_pixels

    return accuracy

def calculate_accuracy(y_true, y_pred, threshold=0.5):
    # Convert predicted probabilities to binary masks using a threshold
    y_pred_binary = (y_pred > threshold).astype(int)

    # Flatten the arrays to facilitate pixel-wise comparison
    y_true_flat = y_true.flatten()
    y_pred_flat = y_pred_binary.flatten()

    # Calculate the proportion of correctly predicted pixels
    accuracy = np.mean(y_true_flat == y_pred_flat)

    return accuracy

# Assuming you have computed the model predictions (result) and ground truth (y)
miss_rate = calculate_miss_rate(y, result)
specificity = calculate_specificity(y, result)
npv_score = calculate_npv(y, result)
fall_out = calculate_fall_out(y, result)
accuracy = pixel_accuracy(y, result)
print("Miss Rate:", miss_rate)
print("Specificity:", specificity)
print("Negative Predictive Value (NPV):", npv_score)
print("Fall-Out:", fall_out)
print("Pixel-wise Accuracy:", accuracy)
print("Model Accuracy:", calculate_accuracy(y, result))

# Initialize variables to store total dice coefficient and number of batches
total_dice_coef = 0.0
num_batches = 0

# Iterate over each batch in the test generator
for batch_index in range(test_steps):
    # Get the batch of test data
    x_test, y_test = test_gen.__getitem__(batch_index)

    # Predict segmentation masks using the trained model
    pred_masks = model.predict(x_test)

    # Calculate the Dice coefficient for the current batch
    batch_dice_coef = dice_coef(tf.cast(y_test, tf.float32), tf.cast(pred_masks, tf.float32)).numpy()

    # Accumulate the total Dice coefficient and increment the batch count
    total_dice_coef += batch_dice_coef
    num_batches += 1

# Calculate the average Dice coefficient across all batches
average_dice_coef = total_dice_coef / num_batches

# Print the average Dice coefficient (which serves as the accuracy)
print("Average Dice Coefficient (Test Accuracy):", average_dice_coef)

def calculate_iou(y_true, y_pred):
    intersection = np.sum(np.logical_and(y_true, y_pred))
    union = np.sum(np.logical_or(y_true, y_pred))
    iou = intersection / (union + 1e-6)  # Add a small epsilon to avoid division by zero
    return iou

# Initialize variables to store total IoU and count of images
total_iou = 0
image_count = 0

# Iterate over test images
for i in range(test_steps):
    # Dataset for prediction
    x, y_true = test_gen.__getitem__(i)

    # Predict segmentation masks using the trained model
    y_pred = model.predict(x) > 0.2

    # Calculate IoU for each image
    for j in range(len(y_true)):
        single_iou = calculate_iou(y_true[j], y_pred[j])
        print(f"IoU for image {image_count + 1}: {single_iou}")
        total_iou += single_iou
        image_count += 1

# Calculate mean IoU
mean_iou = total_iou / image_count
print(f"Mean IoU for {image_count} images: {mean_iou}")

def jaccard_coef(y_true, y_pred):
    y_true = K.cast(y_true, 'float32')
    y_pred = K.cast(y_pred, 'float32')

    intersection = K.sum(y_true * y_pred, axis=-1)
    union = K.sum(y_true, axis=-1) + K.sum(y_pred, axis=-1) - intersection

    return (intersection + smooth) / (union + smooth)

jaccard_scores = []

for i in range(test_steps):
    # Dataset for prediction
    x, y_true = test_gen.__getitem__(i)

    # Predict segmentation masks using the trained model
    y_pred = model.predict(x)
    y_pred = y_pred > 0.2

    for i in range(len(y_pred)):
        jaccard_score = jaccard_coef(y_true[i], y_pred[i])
        jaccard_scores.append(jaccard_score)

# Calculate the average Jaccard coefficient
average_jaccard = np.mean(jaccard_scores)
print("Average Jaccard Coefficient:", average_jaccard)

#----feature extraction ------#

from tensorflow.keras.models import Model
import numpy as np

# Choose a valid layer name from the model summary
layer_name = 'conv2d_114'  # Replace with the actual name of a Conv2D layer within a conv_block

# Create a new model that outputs the selected layer's activations
feature_extractor = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)

# Load an image (Make sure it's preprocessed in the same way as training images)
# Instead of X_test, use X which was likely populated from test_gen.__getitem__(0)
image = X[0]  # Select an image from the test set you loaded earlier using test_gen
image = np.expand_dims(image, axis=0)  # Add batch dimension if needed

# Get feature maps
feature_maps = feature_extractor.predict(image)

# Print the shape of feature maps
print(f"Feature map shape from {layer_name}: {feature_maps.shape}")

# ... (previous code) ...

# Visualizing the filters (no biases for layers with use_bias=False)
layer_name = 'conv2d_114'
filters = model.get_layer(layer_name).get_weights()[0]  # Get only the filters

n_filters = 6  # Visualize the first 6 filters

fig, axes = plt.subplots(1, n_filters, figsize=(15, 5))
for i in range(n_filters):
    ax = axes[i]
    ax.imshow(filters[:, :, :, i], cmap='gray')
    ax.axis("off")

plt.suptitle("Conv2D Layer Filters")
plt.show()

# ... (rest of the code) ...

#-------CLASSIFICATION--------------#

import os
import shutil
from ultralytics import YOLO
import pandas as pd

# Define paths for the dataset
base_path = "/content/drive/MyDrive/retinopatia diabetica.v1i.yolov8"  # Base path where 'train', 'val', 'test' folders are located
model_path = "/content/drive/MyDrive/DREyolov8_experiment/content/runs/detect/train2/weights/best.pt"  # Path to your trained YOLOv8 model

# Define new path for saving images in separate folders (Severe and Moderate)
separate_dir = "/content/drive/MyDrive/DR-classification"  # New directory for classification
severe_folder = os.path.join(separate_dir, 'Severe')
moderate_folder = os.path.join(separate_dir, 'Moderate')

# Create the directories for Severe and Moderate if they don't exist
os.makedirs(severe_folder, exist_ok=True)
os.makedirs(moderate_folder, exist_ok=True)

# Load the YOLOv8 model
model = YOLO(model_path)

# Function to classify severity based on YOLOv8 predictions
def classify_severity(image_path):
    # Run inference on the image
    results = model(image_path)  # Perform inference

    # Access detection boxes
    boxes = results[0].boxes

    # Extract the bounding boxes, class labels, and confidence scores
    xyxy = boxes.xyxy.cpu().numpy()  # Get the coordinates of the boxes in [x1, y1, x2, y2] format
    conf = boxes.conf.cpu().numpy()  # Confidence scores
    cls = boxes.cls.cpu().numpy()  # Class labels

    # Convert to a pandas DataFrame
    detections = pd.DataFrame({
        'x1': xyxy[:, 0],
        'y1': xyxy[:, 1],
        'x2': xyxy[:, 2],
        'y2': xyxy[:, 3],
        'confidence': conf,
        'class': cls
    })

    # Apply confidence threshold (optional, can be adjusted)
    detections = detections[detections['confidence'] > 0.5]  # Confidence threshold

    # Classify based on the detected class labels
    severity = "Moderate"  # Default severity

    # Check if any severe classes (class 0) are detected
    severe_classes = [0]  # Class 0 is for "Severe"

    if any(cls in severe_classes for cls in detections['class'].unique()):
        severity = "Severe"

    return severity

# Loop through train, val, and test subfolders for both images and labels
def process_data():
    severe_images = []  # List to store severe images
    moderate_images = []  # List to store moderate images

    for subset in ['train', 'valid', 'test']:  # Adjust subsets accordingly
        image_folder = os.path.join(base_path, subset, 'images')  # Subfolder with images

        # Loop through images in the current subset
        for image_name in os.listdir(image_folder):
            image_path = os.path.join(image_folder, image_name)  # Full image path

            # Classify image based on YOLOv8 predictions
            severity = classify_severity(image_path)

            # Append the image name to the appropriate list
            if severity == "Severe":
                severe_images.append(image_name)
            else:
                moderate_images.append(image_name)

    # Return the classified image lists
    return severe_images, moderate_images

# Function to save images to separate folders
def save_images(severe_images, moderate_images):
    for subset in ['train', 'valid', 'test']:  # Iterate through subsets
        # Save severe images to the separate severe folder
        for image_name in severe_images:
            image_path = os.path.join(base_path, subset, 'images', image_name)  # Original image path
            destination_path = os.path.join(severe_folder, image_name)  # Destination path in the separate directory
            if os.path.exists(image_path):  # Check if image exists
                shutil.copy(image_path, destination_path)  # Save image to Severe folder in the separate directory

        # Save moderate images to the separate moderate folder
        for image_name in moderate_images:
            image_path = os.path.join(base_path, subset, 'images', image_name)  # Original image path
            destination_path = os.path.join(moderate_folder, image_name)  # Destination path in the separate directory
            if os.path.exists(image_path):  # Check if image exists
                shutil.copy(image_path, destination_path)  # Save image to Moderate folder in the separate directory

# Main function to process data and save images to separate folders
def main():
    severe_images, moderate_images = process_data()

    # Show the result
    print(f"Total Severe Images: {len(severe_images)}")
    print(f"Severe Images: {severe_images}")
    print(f"Total Moderate Images: {len(moderate_images)}")
    print(f"Moderate Images: {moderate_images}")

    # Save the images to their respective folders
    save_images(severe_images, moderate_images)

    print("Images have been successfully saved to 'Severe' and 'Moderate' folders.")

# Run the main function
main()

import os
import shutil
from sklearn.model_selection import train_test_split

# Define paths for the dataset
base_path = "/content/drive/MyDrive/DR-classification"
severe_folder = os.path.join(base_path, "Severe")  # Folder containing all severe images
moderate_folder = os.path.join(base_path, "Moderate")  # Folder containing all moderate images

# Create a new directory to store the train, validation, and test sets
new_base_path = "/content/drive/MyDrive/DR-YOLO-CLASSIFICATION-DATASET"  # New base path
os.makedirs(new_base_path, exist_ok=True)  # Create the new base directory

# Get lists of image names in the 'severe_images' and 'moderate_images' folders
severe_images = [image for image in os.listdir(severe_folder) if image.endswith(('.jpg', '.png'))]
moderate_images = [image for image in os.listdir(moderate_folder) if image.endswith(('.jpg', '.png'))]

# Split the images into train, validation, and test sets (70%, 15%, 15%)
train_severe, temp_severe = train_test_split(severe_images, test_size=0.3, random_state=42)
val_severe, test_severe = train_test_split(temp_severe, test_size=0.5, random_state=42)

train_moderate, temp_moderate = train_test_split(moderate_images, test_size=0.3, random_state=42)
val_moderate, test_moderate = train_test_split(temp_moderate, test_size=0.5, random_state=42)

# Create the directories in the new base path
os.makedirs(os.path.join(new_base_path, "train/severe"), exist_ok=True)
os.makedirs(os.path.join(new_base_path, "train/moderate"), exist_ok=True)
os.makedirs(os.path.join(new_base_path, "val/severe"), exist_ok=True)
os.makedirs(os.path.join(new_base_path, "val/moderate"), exist_ok=True)
os.makedirs(os.path.join(new_base_path, "test/severe"), exist_ok=True)
os.makedirs(os.path.join(new_base_path, "test/moderate"), exist_ok=True)

# Function to copy images to the correct folder
def copy_images_to_folder(images, severity, subset):
    for image_name in images:
        if severity == "Severe":
            source_path = os.path.join(severe_folder, image_name)
            dest_path = os.path.join(new_base_path, subset, "severe", image_name)
        else:
            source_path = os.path.join(moderate_folder, image_name)
            dest_path = os.path.join(new_base_path, subset, "moderate", image_name)

        if os.path.exists(source_path):  # Check if the image exists
            shutil.copy(source_path, dest_path)  # Copy image to new location

# Copy images to their respective folders
copy_images_to_folder(train_severe, "Severe", "train")
copy_images_to_folder(val_severe, "Severe", "val")
copy_images_to_folder(test_severe, "Severe", "test")

copy_images_to_folder(train_moderate, "Moderate", "train")
copy_images_to_folder(val_moderate, "Moderate", "val")
copy_images_to_folder(test_moderate, "Moderate", "test")

# Print the number of images in each folder
def print_image_counts():
    for subset in ["train", "val", "test"]:
        for severity in ["severe", "moderate"]:
            folder_path = os.path.join(new_base_path, subset, severity)
            num_images = len([f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.png'))])
            print(f"{subset.capitalize()} {severity.capitalize()}: {num_images} images")

# Print the image counts for each folder
print_image_counts()

print("Dataset split and images saved successfully!")

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import os
import matplotlib.pyplot as plt
import PIL
import seaborn as sns
import plotly
import plotly.graph_objs as go
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from plotly.offline import iplot, init_notebook_mode
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.initializers import glorot_uniform
from tensorflow.keras.utils import plot_model
from IPython.display import display
from tensorflow.keras import backend as K
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler

!pip install jupyterthemes

from jupyterthemes import jtplot
jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)
# setting the style of the notebook to be monokai theme
# this line of code is important to ensure that we are able to see the x and y axes clearly
# If you don't run this code line, you will notice that the xlabel and ylabel on any plot is black on black and it will be hard to see them.

import os

# Define the base dataset directory
dataset_directory = '/content/drive/MyDrive/DR-CLASSIFICATION-DATASET-20250217T082239Z-001/DR-YOLO-CLASSIFICATION-DATASET'  # Change this to the correct path

# Function to get and format the filenames for a given directory
def get_image_filenames(directory):
    # List all files in the directory
    image_files = [f for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]  # Adjust file extensions if needed
    # Format the filenames as requested
    formatted_image_list = ", ".join([f"'{image}'" for image in image_files])
    return formatted_image_list

# Directories for train, validation, and test images (both severe and moderate)
train_severe_dir = os.path.join(dataset_directory, 'train', 'severe')
train_moderate_dir = os.path.join(dataset_directory, 'train', 'moderate')
val_severe_dir = os.path.join(dataset_directory, 'val', 'severe')
val_moderate_dir = os.path.join(dataset_directory, 'val', 'moderate')
test_severe_dir = os.path.join(dataset_directory, 'test', 'severe')
test_moderate_dir = os.path.join(dataset_directory, 'test', 'moderate')

# Get and print filenames for each directory
print("Train Severe Images:", get_image_filenames(train_severe_dir))
print("Train Moderate Images:", get_image_filenames(train_moderate_dir))
print("Validation Severe Images:", get_image_filenames(val_severe_dir))
print("Validation Moderate Images:", get_image_filenames(val_moderate_dir))
print("Test Severe Images:", get_image_filenames(test_severe_dir))
print("Test Moderate Images:", get_image_filenames(test_moderate_dir))

import os

# Define the base dataset directory
dataset_directory = '/content/drive/MyDrive/DR-CLASSIFICATION-DATASET-20250217T082239Z-001/DR-YOLO-CLASSIFICATION-DATASET'  # Replace with your actual path

# Function to count the number of images in a given directory
def count_images_in_directory(directory):
    # List all files in the directory and count the ones that are images (e.g., .jpg, .png)
    image_files = [f for f in os.listdir(directory) if f.endswith('.png') or f.endswith('.jpg')]  # Modify extensions as needed
    return len(image_files)

# Directories for train, validation, and test images (both severe and moderate)
train_severe_dir = os.path.join(dataset_directory, 'train', 'severe')
train_moderate_dir = os.path.join(dataset_directory, 'train', 'moderate')
val_severe_dir = os.path.join(dataset_directory, 'val', 'severe')
val_moderate_dir = os.path.join(dataset_directory, 'val', 'moderate')
test_severe_dir = os.path.join(dataset_directory, 'test', 'severe')
test_moderate_dir = os.path.join(dataset_directory, 'test', 'moderate')

# Count the number of images in each directory
train_severe_count = count_images_in_directory(train_severe_dir)
train_moderate_count = count_images_in_directory(train_moderate_dir)
val_severe_count = count_images_in_directory(val_severe_dir)
val_moderate_count = count_images_in_directory(val_moderate_dir)
test_severe_count = count_images_in_directory(test_severe_dir)
test_moderate_count = count_images_in_directory(test_moderate_dir)

# Print the counts
print(f"Train Severe Images: {train_severe_count} images")
print(f"Train Moderate Images: {train_moderate_count} images")
print(f"Validation Severe Images: {val_severe_count} images")
print(f"Validation Moderate Images: {val_moderate_count} images")
print(f"Test Severe Images: {test_severe_count} images")
print(f"Test Moderate Images: {test_moderate_count} images")

#-------LABEL MAPPING--------#

import os

# Define dataset directories
dataset_directory = '/content/drive/MyDrive/DR-CLASSIFICATION-DATASET-20250217T082239Z-001/DR-YOLO-CLASSIFICATION-DATASET'  # Replace with your actual path

# Define label mapping based on folder structure
label_mapping = {
    'severe': 'Severe',
    'moderate': 'Moderate'
}

# Function to get image labels from folder names
def get_labels(directory):
    labels = []
    for category in [ 'moderate', 'severe']:  # Ensure to include all categories in dataset
        category_dir = os.path.join(directory, category)
        if os.path.exists(category_dir):
            num_images = len([f for f in os.listdir(category_dir) if f.endswith('.png') or f.endswith('.jpg')])
            labels.extend([label_mapping[category]] * num_images)
    return labels

# Get labels from dataset (for train, val, test)
train_labels = get_labels(os.path.join(dataset_directory, 'train'))
val_labels = get_labels(os.path.join(dataset_directory, 'val'))
test_labels = get_labels(os.path.join(dataset_directory, 'test'))

# Print labels in required format
print("\n".join(train_labels))
print("\n".join(val_labels))
print("\n".join(test_labels))

import os
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Define dataset directory
dataset_directory = "/content/drive/MyDrive/DR-CLASSIFICATION-DATASET-20250217T082239Z-001/DR-YOLO-CLASSIFICATION-DATASET"  # Replace with your actual dataset path

# Function to count images in Severe and Moderate categories
def count_images(directory):
    class_counts = {"Severe": 0, "Moderate": 0}

    for category in class_counts.keys():
        category_dir = os.path.join(directory, category.lower())  # Ensure folder names are lowercase
        if os.path.exists(category_dir):
            num_images = len([f for f in os.listdir(category_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])
            class_counts[category] = num_images

    return class_counts

# Get class distribution from all datasets
train_counts = count_images(os.path.join(dataset_directory, "train"))
val_counts = count_images(os.path.join(dataset_directory, "val"))
test_counts = count_images(os.path.join(dataset_directory, "test"))

# Combine counts for full dataset visualization
total_counts = {
    "Train": train_counts,
    "Validation": val_counts,
    "Test": test_counts
}

# Convert to a format suitable for Seaborn
data = []
for dataset, counts in total_counts.items():
    for category, count in counts.items():
        data.append({"Dataset": dataset, "Class": category, "Count": count})

df = pd.DataFrame(data)

# Plot using Seaborn
plt.figure(figsize=(8, 6))
sns.barplot(x="Class", y="Count", hue="Dataset", data=df, palette="pastel")
plt.title("Severe & Moderate Class Distribution Across Datasets")
plt.xlabel("Class")
plt.ylabel("Number of Images")
plt.legend(title="Dataset")
plt.show()

#------VISUALIZATION------#

import os
import random
import matplotlib.pyplot as plt
import PIL.Image

# Define dataset path
dataset_directory = "/content/drive/MyDrive/DR-CLASSIFICATION-DATASET-20250217T082239Z-001/DR-YOLO-CLASSIFICATION-DATASET"  # Update with the actual dataset path

# Define subsets and classes
subset = "train"  # Change to 'val' or 'test' if needed
classes = ["severe", "moderate"]  # Folders inside 'train', 'val', 'test'

# Create subplots (2 rows, 5 columns)
fig, axs = plt.subplots(len(classes), 5, figsize=(20, 10))

# Loop through each class
for count, cls in enumerate(classes):
    class_path = os.path.join(dataset_directory, subset, cls)
    images = [img for img in os.listdir(class_path) if img.endswith(('.png', '.jpg', '.jpeg'))]

    # Select 5 random images
    random_images = random.sample(images, min(5, len(images)))

    # Display images
    for j, img_name in enumerate(random_images):
        img_path = os.path.join(class_path, img_name)
        img = PIL.Image.open(img_path)

        axs[count][j].imshow(img)
        axs[count][j].axis("on")  # Add axis lines
        axs[count][j].title.set_text(f"{cls.capitalize()} ({subset})")  # Title: 'Severe (train)', 'Moderate (train)'

# Adjust layout
plt.tight_layout()
plt.show()

import os

# Path to training dataset
train_path = "/content/drive/MyDrive/DR-CLASSIFICATION-DATASET-20250217T082239Z-001/DR-YOLO-CLASSIFICATION-DATASET/train"  # Update this path

# Count images in each class
classes = ["severe", "moderate"]  # Modify if you have more classes
class_counts = {}

for cls in classes:
    class_folder = os.path.join(train_path, cls)
    num_images = len(os.listdir(class_folder))
    class_counts[cls] = num_images

# Print the count of images in each class
for cls, count in class_counts.items():
    print(f"{cls.capitalize()} Class: {count} images")

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define paths
dataset_path = "/content/drive/MyDrive/DR-CLASSIFICATION-DATASET-20250217T082239Z-001/DR-YOLO-CLASSIFICATION-DATASET"  # Update this path

# Define batch size and image size
BATCH_SIZE = 8
IMAGE_SIZE = (256, 256)  # Adjust based on your model requirements

# Data Augmentation & Normalization
train_datagen = ImageDataGenerator(
    rescale=1./255,      # Normalize pixel values
    rotation_range=20,   # Randomly rotate images
    width_shift_range=0.2,  # Randomly shift images horizontally
    height_shift_range=0.2, # Randomly shift images vertically
    shear_range=0.2,     # Apply shearing
    zoom_range=0.2,      # Apply zooming
    horizontal_flip=True # Flip images
)

# Only rescale for validation & test sets (No augmentation)
val_test_datagen = ImageDataGenerator(rescale=1./255)

# Load training dataset
train_generator = train_datagen.flow_from_directory(
    directory=os.path.join(dataset_path, "train"),
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="binary",  # Use 'categorical' if more than 2 classes
    shuffle=True
)

# Load validation dataset
val_generator = val_test_datagen.flow_from_directory(
    directory=os.path.join(dataset_path, "val"),
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="binary",
    shuffle=False
)

# Load test dataset
test_generator = val_test_datagen.flow_from_directory(
    directory=os.path.join(dataset_path, "test"),
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="binary",
    shuffle=False
)

# Print class labels
print("Class Indices:", train_generator.class_indices)

#---------RESNET 18 CLASSIFICATION MODEL----------#

from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Add, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Flatten, Dense, Dropout, ZeroPadding2D
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.initializers import glorot_uniform

def res_block(X, filters, stage, dropout_rate=0.3, l2_lambda=0.01):
    X_copy = X
    f1, f2, f3 = filters

    # Main Path
    X = Conv2D(f1, (1,1), strides=(1,1), kernel_initializer=glorot_uniform(seed=0),
               kernel_regularizer=l2(l2_lambda), name=f'res_{stage}_conv_a')(X)
    X = BatchNormalization(name=f'bn_{stage}_conv_a')(X)
    X = Activation('relu')(X)

    X = Conv2D(f2, (3,3), strides=(1,1), padding='same', kernel_initializer=glorot_uniform(seed=0),
               kernel_regularizer=l2(l2_lambda), name=f'res_{stage}_conv_b')(X)
    X = BatchNormalization(name=f'bn_{stage}_conv_b')(X)
    X = Activation('relu')(X)

    X = Conv2D(f3, (1,1), strides=(1,1), kernel_initializer=glorot_uniform(seed=0),
               kernel_regularizer=l2(l2_lambda), name=f'res_{stage}_conv_c')(X)
    X = BatchNormalization(name=f'bn_{stage}_conv_c')(X)

    # Short Path
    X_copy = Conv2D(f3, (1,1), strides=(1,1), kernel_initializer=glorot_uniform(seed=0),
                    kernel_regularizer=l2(l2_lambda), name=f'res_{stage}_conv_copy')(X_copy)
    X_copy = BatchNormalization(name=f'bn_{stage}_conv_copy')(X_copy)

    # Add
    X = Add()([X, X_copy])
    X = Activation('relu')(X)
    X = Dropout(dropout_rate)(X)  # Dropout after residual connection

    return X

# Input shape
input_shape = (256,256,3)
X_input = Input(input_shape)

# Zero-padding
X = ZeroPadding2D((3,3))(X_input)

# 1st stage
X = Conv2D(64, (7,7), strides=(2,2), kernel_initializer=glorot_uniform(seed=0),
           kernel_regularizer=l2(0.01), name='conv1')(X)
X = BatchNormalization(name='bn_conv1')(X)
X = Activation('relu')(X)
X = MaxPooling2D((3,3), strides=(2,2))(X)

# 2nd stage
X = res_block(X, filters=[64,64,256], stage=2, dropout_rate=0.3, l2_lambda=0.01)

# 3rd stage
X = res_block(X, filters=[128,128,512], stage=3, dropout_rate=0.4, l2_lambda=0.01)

# 4th stage
X = res_block(X, filters=[256,256,1024], stage=4, dropout_rate=0.5, l2_lambda=0.01)

# Global Average Pooling
X = GlobalAveragePooling2D(name='Global_Avg_Pooling')(X)

# Fully connected layer with dropout
X = Dense(256, activation='relu', kernel_initializer=glorot_uniform(seed=0),
          kernel_regularizer=l2(0.01), name='Dense_1')(X)
X = Dropout(0.5)(X)

# Output layer
X = Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=0),
          kernel_regularizer=l2(0.01), name='Dense_final')(X)

# Model creation
model = Model(inputs=X_input, outputs=X, name='ResNet18_Regularized')

# Compile model
optimizer = Adam(learning_rate=0.001, decay=1e-4)
model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Callbacks to prevent overfitting
callbacks = [
    #EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)
]

# Model summary
model.summary()

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // BATCH_SIZE,
    epochs=300,  # Adjust the number of epochs as needed
    validation_data=val_generator,
    validation_steps=val_generator.samples // BATCH_SIZE,
    callbacks=callbacks,
    verbose=1
)

model.save("/content/drive/MyDrive/resnet18_classification-300.h5")

import matplotlib.pyplot as plt

# Plot training & validation accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()

# Plot training & validation loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['Train', 'Val'], loc='upper left')
plt.show()



import numpy as np
import matplotlib.pyplot as plt
import os
from tensorflow.keras.preprocessing import image

# Load the saved model
model_path = "/content/drive/MyDrive/resnet18_classification-300.h5"  # Update with your model path
model.load_weights(model_path)

# Image size
IMAGE_SIZE = (250, 250)

# Randomly select images from the test dataset
num_samples = 20  # Number of images to display (each will have its prediction beside it)
indices = np.random.choice(len(test_generator.filenames), num_samples, replace=False)

# Initialize lists for images, true labels, and predicted labels
original_images = []
true_classes = []
predicted_classes = []

# Load and preprocess selected images
for idx in indices:
    img_path = os.path.join(test_generator.directory, test_generator.filenames[idx])

    # Load original image
    img = image.load_img(img_path, target_size=IMAGE_SIZE)
    img_array = image.img_to_array(img) / 255.0  # Normalize
    original_images.append(img_array)
    true_classes.append(test_generator.classes[idx])

# Convert list to NumPy array
original_images = np.array(original_images)

# Make predictions
predictions = model.predict(original_images)
predicted_classes = (predictions > 0.5).astype(int).flatten()

# Get class labels from the generator
class_indices = test_generator.class_indices
class_labels = {v: k for k, v in class_indices.items()}  # Reverse mapping

# Plot images with original and predicted side by side
fig, axes = plt.subplots(num_samples, 2, figsize=(10, num_samples * 5))

for i in range(num_samples):
    true_label = class_labels[true_classes[i]]
    pred_label = class_labels[predicted_classes[i]]
    color = "green" if true_label == pred_label else "red"

    # Original Image (Left Side)
    ax1 = axes[i, 0]
    ax1.imshow(original_images[i])
    ax1.set_xticks(np.linspace(0, 250, 6))  # Set x-axis ticks
    ax1.set_yticks(np.linspace(0, 250, 6))  # Set y-axis ticks
    ax1.set_xlabel("Pixels")
    ax1.set_ylabel("Pixels")
    ax1.set_title(f"Original\nClass: {true_label}", color="blue")

    # Predicted Image (Right Side)
    ax2 = axes[i, 1]
    ax2.imshow(original_images[i])  # Same image for reference
    ax2.set_xticks(np.linspace(0, 250, 6))  # Set x-axis ticks
    ax2.set_yticks(np.linspace(0, 250, 6))  # Set y-axis ticks
    ax2.set_xlabel("Pixels")
    ax2.set_ylabel("Pixels")
    ax2.set_title(f"Predicted\nClass: {pred_label}", color=color)

plt.tight_layout()
plt.show()

import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Get the true labels and predicted labels
y_true = []
y_pred = []

# Iterate over the test dataset to get predictions
for batch_images, batch_labels in test_generator:
    preds = model.predict(batch_images)  # Get predictions
    preds = (preds > 0.5).astype(int).flatten()  # Convert to binary (0/1)

    y_true.extend(batch_labels.astype(int).flatten())  # True labels
    y_pred.extend(preds)  # Predicted labels

    if len(y_true) >= test_generator.samples:  # Ensure we don't exceed dataset size
        break

# Convert to NumPy arrays
y_true = np.array(y_true)
y_pred = np.array(y_pred)

# Compute accuracy
accuracy = accuracy_score(y_true, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")

# Generate classification report (Precision, Recall, F1-score)
print("\nClassification Report:\n", classification_report(y_true, y_pred, target_names=list(test_generator.class_indices.keys())))

# Confusion Matrix
conf_matrix = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=test_generator.class_indices.keys(), yticklabels=test_generator.class_indices.keys())
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, log_loss, matthews_corrcoef
import seaborn as sns
import matplotlib.pyplot as plt

# Get the true labels and predicted labels
y_true = []
y_pred = []
y_prob = []  # Store probability scores for ROC-AUC

# Iterate over test dataset
for batch_images, batch_labels in test_generator:
    preds_prob = model.predict(batch_images)  # Get probability scores
    preds = (preds_prob > 0.5).astype(int).flatten()  # Convert to binary (0/1)

    y_true.extend(batch_labels.astype(int).flatten())  # True labels
    y_pred.extend(preds)  # Predicted labels
    y_prob.extend(preds_prob.flatten())  # Store probabilities for ROC-AUC

    if len(y_true) >= test_generator.samples:  # Ensure we don't exceed dataset size
        break

# Convert to NumPy arrays
y_true = np.array(y_true)
y_pred = np.array(y_pred)
y_prob = np.array(y_prob)

# Calculate Metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, average="binary")  # Change to "macro" for multi-class
recall = recall_score(y_true, y_pred, average="binary")
f1 = f1_score(y_true, y_pred, average="binary")
specificity = confusion_matrix(y_true, y_pred)[0, 0] / sum(confusion_matrix(y_true, y_pred)[0])  # TN / (TN + FP)
roc_auc = roc_auc_score(y_true, y_prob)
logloss = log_loss(y_true, y_prob)
mcc = matthews_corrcoef(y_true, y_pred)

# Print Results
print(f"🔹 Accuracy: {accuracy:.4f}")
print(f"🔹 Precision: {precision:.4f}")
print(f"🔹 Recall (Sensitivity): {recall:.4f}")
print(f"🔹 F1-Score: {f1:.4f}")
print(f"🔹 Specificity: {specificity:.4f}")
print(f"🔹 ROC-AUC Score: {roc_auc:.4f}")
print(f"🔹 Log Loss: {logloss:.4f}")
print(f"🔹 Matthews Correlation Coefficient (MCC): {mcc:.4f}")

# Confusion Matrix
#conf_matrix = confusion_matrix(y_true, y_pred)
#plt.figure(figsize=(6, 4))
#sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
#plt.xlabel("Predicted Label")
#plt.ylabel("True Label")
#plt.title("Confusion Matrix")
plt.show()